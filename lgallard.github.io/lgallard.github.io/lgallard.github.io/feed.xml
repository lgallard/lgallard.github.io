<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.0">Jekyll</generator><link href="https://lgallardo.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lgallardo.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/</id><title type="html">lgallardo.com</title><subtitle>DevOps Engineer and Backend Solutions Developer specializing in Kubernetes, AWS, Python, and Terraform. Sharing insights on cloud infrastructure, automation, and system architecture.</subtitle><author><name>Luis M. Gallardo D.</name></author><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot.md/">![BugBot and Copilot: AI Reviewers](/assets/images/coding-agents-bugbot-ac817363-8710-4b8f-a6a5-15e47cae8f8c.png){:style=&quot;display:block; margin-left:auto; margin-right:auto; width:60%&quot;}

Recently, while working on my Terraform modules, I had an idea: **What about using BugBot to check Copilot's Coding Agents?** With Copilot Coding Agents already helping automate PRs and refactors, I decided to give BugBot a try as an additional reviewer.

## The Experiment

I assigned Copilot Coding Agents to implement new features and fixes in my repositories. Once the PRs were ready, I ran BugBot to review the changes. The results were fascinating:

![Copilot PR with BugBot review](/assets/images/coding-agents-bugbot-2025-06-09_00-46.png)

BugBot quickly detected issues that Copilot's own review had missed, especially around edge cases and variable usage. Its bug detection capabilities are on another level compared to Copilot's built-in review features.

![BugBot detailed bug report](/assets/images/coding-agents-bugbot-2025-06-09_00-47.png)

For example, BugBot flagged overly restrictive regex validation and unused variables in a Copilot-generated PR. It provided actionable feedback, which I then relayed to Copilot for fixes.

![BugBot and Copilot: AI Reviewers](/assets/images/coding-agents-bugbot2025-06-09_00-48.png)


## Highlights

1. **BugBot excels at bug detection:** It found subtle issues in Copilot Coding Agents' PRs, providing detailed explanations and suggestions.
2. **Copilot Coding Agents excel at implementation:** They quickly propose and implement solutions, and can interpret and act on BugBot's feedback.
3. **AI is not infallible:** Even with both tools, bugs or outdated code can slip through. For example, Copilot generated a workflow using a deprecated GitHub Action, and BugBot didn't catch it either.

![Workflow error: Deprecated GitHub Action](/assets/images/coding-agents-bugbot-2025-06-09_01-01.png)


## Conclusion

Combining Copilot Coding Agents and BugBot creates a powerful AI-driven review and implementation workflow. Each tool has its strengths: BugBot for deep bug detection, Copilot for rapid solution delivery. However, human oversight is still essential to catch edge cases and evolving platform changes.


&gt; **Note:** BugBot is a paid service for automated code reviews. My tests and experiments described here were conducted during the free trial period, which allowed me to evaluate its capabilities without incurring any cost.


## References

- [GitHub Copilot](https://github.com/features/copilot)
- [Cursor BugBot](https://www.cursor.so/bugbot) 
- [feat: Add support for repository replication ](https://github.com/lgallard/terraform-aws-ecr/pull/55)</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2025-06-08-coding-agents-bugbot/">&lt;p&gt;&lt;img src=&quot;/assets/images/coding-agents-bugbot-ac817363-8710-4b8f-a6a5-15e47cae8f8c.png&quot; alt=&quot;BugBot and Copilot: AI Reviewers&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recently, while working on my Terraform modules, I had an idea: &lt;strong&gt;What about using BugBot to check Copilot’s Coding Agents?&lt;/strong&gt; With Copilot Coding Agents already helping automate PRs and refactors, I decided to give BugBot a try as an additional reviewer.&lt;/p&gt;

&lt;h2 id=&quot;the-experiment&quot;&gt;The Experiment&lt;/h2&gt;

&lt;p&gt;I assigned Copilot Coding Agents to implement new features and fixes in my repositories. Once the PRs were ready, I ran BugBot to review the changes. The results were fascinating:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/coding-agents-bugbot-2025-06-09_00-46.png&quot; alt=&quot;Copilot PR with BugBot review&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BugBot quickly detected issues that Copilot’s own review had missed, especially around edge cases and variable usage. Its bug detection capabilities are on another level compared to Copilot’s built-in review features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/coding-agents-bugbot-2025-06-09_00-47.png&quot; alt=&quot;BugBot detailed bug report&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, BugBot flagged overly restrictive regex validation and unused variables in a Copilot-generated PR. It provided actionable feedback, which I then relayed to Copilot for fixes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/coding-agents-bugbot2025-06-09_00-48.png&quot; alt=&quot;BugBot and Copilot: AI Reviewers&quot; /&gt;&lt;/p&gt;</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2025-06-05-github-copilot-terraform-aws-backup.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2025-06-05-github-copilot-terraform-aws-backup.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2025-06-05-github-copilot-terraform-aws-backup.md/">&lt;!-- Add a relevant image here --&gt;
![GitHub Copilot Background Agents](/assets/images/copilot-coding-agent.png){:style=&quot;display:block; margin-left:auto; margin-right:auto;}

Recently, I had the opportunity to try out GitHub Copilot Coding Agents (Background Agents) on one of my personal projects: the [terraform-aws-backup](https://github.com/lgallard/terraform-aws-backup) module. This module is designed to help users manage AWS Backup plans using Terraform.

## The Problem

A user opened [an issue](https://github.com/lgallard/terraform-aws-backup/issues/114) requesting support for multiple backup plans per vault. This required a significant refactor of the module, as it previously only supported a single plan per vault. I decided to assign the issue to Copilot to see how well it could handle a real-world infrastructure-as-code (IaC) refactor.

![GitHub Issue Assignment](/assets/images/copilot-assigned-by-lgallard.png)

## The Copilot Solution

Copilot Background Agents picked up the issue and proposed a pull request ([PR #115](https://github.com/lgallard/terraform-aws-backup/pull/115)) that introduced a new `plans` variable (a map of maps) to support multiple backup plans. The PR included all necessary code changes, documentation updates, and maintained backward compatibility with the previous single-plan approach.

I interacted with Copilot directly through GitHub, asking questions and requesting changes. For example, I verified backward compatibility and reported an edge case related to Terraform state migration. Copilot responded to my comments, addressed the issues, and updated the PR accordingly.

![Copilot PR Conversation](/assets/images/copilot-pr.png)

## Highlights

- **End-to-end automation:** Most of the process happened within GitHub, with minimal human intervention.
- **Speed:** The entire refactor and review process took about 35 minutes, with Copilot handling the bulk of the work in just 13 minutes.
- **Device flexibility:** I managed the process from my iPad mini using Arc, without needing a local IDE.
- **Iterative feedback:** Copilot handled multiple rounds of feedback, including error reports and code adjustments.

## Drawbacks

- **Model selection:** There was no option to choose which AI model Copilot used for the task.
- **Testing required:** As with any automated code generation, manual testing and validation were necessary before merging.
- **Image handling:** Copilot could not read images pasted in comments, so error messages had to be provided as text.

## Conclusion

GitHub Copilot Background Agents proved to be a valuable tool for unattended, automation-friendly tasks like module refactoring. While it's important to test and validate the generated code, Copilot can significantly accelerate development workflows, especially for infrastructure code.

## References

- [terraform-aws-backup module on GitHub](https://github.com/lgallard/terraform-aws-backup)
- [terraform-aws-backup module on Terraform Registry](https://registry.terraform.io/modules/lgallard/backup/aws)
- [GitHub Copilot Background Agents](https://docs.github.com/en/copilot)</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2024-05-12-anbernic-35xxh-batocera.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2024-05-12-anbernic-35xxh-batocera.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2024-05-12-anbernic-35xxh-batocera.md/">![Anbernic 35xxH](/assets/images/anbernic-35xxh.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;
Today, the market is flooded with a variety of handheld devices for retro emulation, originating from several Chinese companies such as [Retroid](https://www.goretroid.com/){:target=&quot;_blank&quot;}, [Powkiddy](https://powkiddy.com/){:target=&quot;_blank&quot;}, and [Anbernic](https://anbernic.com/){:target=&quot;_blank&quot;}. These devices vary in form factors—ranging from compact screens to more powerful computing capabilities and operating systems, some of which are open-source, while others are proprietary and maintained by the manufacturers themselves.

After watching countless YouTube videos, comparing specifications, and exploring retro consoles on AliExpress priced under $100, I stumbled upon the **Anbernic 35xxH**. This model, featuring a 3.5-inch screen, is developed by Anbernic. It is similar to the **Anbernic 35xx Plus** but sports a horizontal orientation. The model name '35xxH' indicates a 3.5-inch screen ('35xx') and a horizontal layout ('H').

As mentioned earlier, some operating systems are developed by the device manufacturers, like Anbernic, which offers a proprietary OS. While this OS serves its purpose, it is not without limitations. For example, the way games are displayed and organized can be confusing and cumbersome, and some emulators perform better on RetroArch than on the standalone emulators included with the OS.

Seeking a more user-friendly interface, I turned to [Batocera](https://batocera.org/){:target=&quot;_blank&quot;}, an open-source project maintained by the community. Batocera provides a more intuitive and visually appealing user interface.

![Anbernic 35xxh](/assets/images/batocera-anbernic-35xxh.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;

For those interested in using Batocera on your **Anbernic 35xxH**, there is a repository where you can download the Batocera image, called [Batocera Lite Beta for RG35XX Plus and RG35XX H](https://github.com/rg35xx-cfw/rg35xx-cfw.github.io/discussions/104){:target=&quot;_blank&quot;}.

**References**
  * &lt;a href=&quot;https://www.goretroid.com/&quot; target=&quot;_blank&quot;&gt;Retroid&lt;/a&gt;
  * &lt;a href=&quot;https://powkiddy.com/&quot; target=&quot;_blank&quot;&gt;Powkiddy&lt;/a&gt;
  * &lt;a href=&quot;https://anbernic.com/&quot; target=&quot;_blank&quot;&gt;Anbernic&lt;/a&gt;
  * &lt;a href=&quot;https://batocera.org/&quot; target=&quot;_blank&quot;&gt;Batocera&lt;/a&gt;</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2023-12-18-aws-certified-solutions-architect-professional-2023.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2023-12-18-aws-certified-solutions-architect-professional-2023.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2023-12-18-aws-certified-solutions-architect-professional-2023.md/">![OctoPrints](/assets/images/aws-csap-2023.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;
This article is to share my experience with the AWS Certified Solutions Architect - Professional (recertification) exam:

If you're taking the exam for the first time or, as in my case, seeking recertification, I recommend that you review the new content and check what has changed compared to the previous exam. Examples of these changes could be AWS SSO to AWS Identity Center. I also recommend that you familiarize yourself with new services such as AWS Elastic Disaster Recovery, EventBridge, Amazon Managed Services for Apache Flink, among others.

There were 75 questions in 180 minutes. As is typical with AWS professional certifications, the exam is long and features extensive statements and lengthy answers. For those whose native language is not English, this can be confusing, so you will likely have to read several questions more than once (and their options).
If you're taking the exam for the first time or, as in my case, seeking recertification, I recommend that you review the new content and check what has changed compared to the previous exam. Examples of these changes could be AWS SSO to AWS Identity Center. I also recommend that you familiarize yourself with new services such as AWS Elastic Disaster Recovery, EventBridge, Amazon Managed Services for Apache Flink, among others.
My recommendations for saving time in this exam are:

* Answer the easy questions quickly; don't spend too much time pondering whether it's the correct answer because it most likely is.
* Mark the questions where you genuinely have doubts, so when reviewing, you can focus only on those. If time remains, then start reviewing the rest.
* Mark multiple-choice questions for review, even if you think they are correct. It's better to be 100% sure of the options because if you make a mistake in one, the answer can be considered incorrect.
* Read the questions carefully because the statement contains the information that will help you eliminate options or choose the correct ones.
* Remember that this is an Amazon certification, so always ask yourself which Amazon service is relevant.
* Dismiss solutions that can be done at the OS level if an Amazon service provides it, only considering it when there is no other option.
* Not everything is solved with Lambda + SQS, even if it seems correct. Review which options or services meet the requirements of the question and don't be misled by options that initially seem correct.

If you are interested in this certification, I recommend some links:

**Online courses**

For this exam, I stayed loyal to AWS A Cloud Guru, but to be honest, it seems to me that the content is covered superficially, and the course as it stands today looks like a Frankenstein creation. There are at least two instructors with different content formats, as if they had merged the best of Linux Academy's courses with those of A Cloud Guru. Nonetheless, if you are interested, here it is available:

  * &lt;a href=&quot;https://www.pluralsight.com/courses/aws-certified-solutions-architect-professional-sap-c02&quot; target=&quot;_blank&quot;&gt;A Cloud Guru - AWS Certified Solutions Architect Professional&lt;/a&gt;



**AWS Documentation**

  * &lt;a href=&quot;https://aws.amazon.com/whitepapers/&quot; target=&quot;_blank&quot;&gt;Whitepapers&lt;/a&gt;
  * &lt;a href=&quot;https://aws.amazon.com/documentation/&quot; target=&quot;_blank&quot;&gt;Documentation&lt;/a&gt;
  * &lt;a href=&quot;https://aws.amazon.com/faqs/&quot; target=&quot;_blank&quot;&gt;FAQs&lt;/a&gt;</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2022-09-25-octoprint-multiple-printers.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2022-09-25-octoprint-multiple-printers.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2022-09-25-octoprint-multiple-printers.md/">![OctoPrints](/assets/images/OctoPrint.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;
One of the most useful tools when managing a 3D printer is [OctoPrint](https://octoprint.org){:target=&quot;_blank&quot;}, since among several things it allows you to manage your printer from a web interface, as well as adding a lot of functionality (for example, monitoring and management of the printer through Telegram, plugins to generate timelapse videos of the prints, or even detect when there are spaghetti-like problems in our prints using another plugin with AI).

## Multiple printers
 But what if we have more than one printer? Can **OctoPrint** manage more than one printer?
 
The short answer is: NO! The software itself is not designed for multiple printers, but you can find alternative ways to have more than one instance of **OctoPrint** running on your device or computer.
 
There are several ways to do this, for example you can [modify the OctoPi scripts to add more instances](http://thomas-messmer.com/index.php/14-free-knowledge/howtos/79-setting-up- octoprint-for-multiple-printers){:target=&quot;_blank&quot;}, create scripts that launch multiple instances of the same program, but the option I ended up using was Docker containers.
 
## Raspberry Pi + Ubuntu
In my case I have a **Raspberry Pi 4**, with 8 GB of RAM, more than enough to run multiple instances of **OctoPrint**.
 
I originally had a specific operating system installed on my Raspberry called [OctoPi](https://github.com/guysoft/OctoPi){:target=&quot;_blank&quot;}, a Debian derivative with **OctoPrint** as the main program that is launched when the Raspberry Pi is turned on, but as I wanted to use containers I decided to use Ubuntu, which general purposed distribution where you can surely find everything you need to work with containers.
 
## Requirements and components:
For this setup I used the following:

* Impresora 3D - Artillery Genius Pro
* Impresora 3D - Artillery SideWinder X2
* 2x WebCam - Logitech C270 (one per printer)
* Raspberry Pi 4 - 8G RAM
* SD Card 64 GB
* Ubuntu 22.04 LTS  ARM 64 bits
* Docker 20.10.17
* OctoPrint lastest (1.8.3)

## Ubuntu Installation
Nowadays is easier than ever to install **Ubuntu** on the Raspberry Pi, you just have to download **Raspberry Pi Imager,** which will generate the image on your SD card, select the operating system, which in this case is **Ubuntu 22.04 LTS** and you're done.  **Raspberry Pi Imager** is avialable for Linux, Mac and Windows, so you can generate the image on the operating system that suits you best.

![Raspberry Pi Imager](/assets/images/Raspberry-Pi-Imager.jpg)

From this very software you will have the possibility to configure the password of the WiFi network in case you are using the Raspberry wirelessly, so you don't need  to use a television set and a keyboard (headless), or manipulate the SD card to configure the networking. Since I'm using it headless, that's what I did and then I connected via **ssh**.

![WiFi Settings](/assets/images/OctoPi-WiFi-Setting.jpg)

For the headless setup the trick is to let the **Ubuntu** initialization scripts run and then connect via ssh. Now if you see that you can't connect via ssh you can look for an HDMI cable and a keyboard to check what might be going on.

## Software installation on Ubuntu
Apart from **Docker** we are going to need **docker-compose**, and the utilities that will help us install them.

## Docker and docker-compose
To install docker you need to download it using the script. You will need to loging in as the administrator user with the root account:

```bash
pi@octopi:~$ sudo su
root@octopi:~# 

root@octopi:~# apt install curl
root@octopi:~# curl -sSL https://get.docker.com | sh

root@octopi:~# apt update
root@octopi:~# apt install docker-compose
```

## OctoPrint with one printer
The first thing you need is to create a folder for OctoPrint, and then go into it:

```bash
root@octopi:~# mkdir octoprint
root@octopi:~# 
root@octopi:~# cd octoprint/
root@octopi:~/octoprint#
```

Let's start with a printer, in this case I'll start with the `Artillery Genius Pro`. For this you can create a folder that identifies this printer:

```bash
root@octopi:~# mkdir geniuspro
root@octopi:~# chmod -R 777 geniuspro/
```

It's important to know the actual path because you'll use it later in the container definition along with the name of this folder. To do this, execute the following command:

```bash
root@octopi:~# pwd
/root/octoprint
```

With this you can indicate the files of this first instance of OctoPrint will reside in `/root/octoprint/geniuspro`

Now to configure the printer you need to use  `docker-compose` so you need to create the `docker-compose.yml` file with the following information:

```
version: '2.4'

services:
  geniuspro:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5000:5000
      - 8080:8080
    devices:
      - /dev/ttyACM0:/dev/ttyACM0
      - /dev/video0:/dev/video0
    volumes:
      - /root/octoprint/geniuspro:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30
```

This file defines various things related to OctoPrint, such as the webcam and the printer itself. For example,  `image`  indicates which Docker image to use for the `geniuspro` service.

Notice the ports definitions, where `5000` will be used for the web interface and `8080` for webcam streaming video.

On the other hand, the printer is defined as a device in `/dev/ttyACM0`, and the webcam in `/dev/video0` 

Something to keep in mind and that differs from the [example on which I based](https://docs.google.com/document/d/1aU7LGYAe6r45LqEBQ8opuXCiNKeJl3XrPTrobvEvdOM/edit){:target=&quot;_bla￼nk&quot;}, is that the streamer is now part of the Docker image and is served on port `8080`, so there is no need to have a separate definition of another Docker image pointing to the streamer.

 `volumes`  has the reference to the printer directory that you had created as `/root/octoprint/geniuspro`, which will be mapped into the container as the `/octoprint` folder. That is, when we launch the container all the OctoPrint files for this printer will be in the `/root/octoprint/geniuspro` path of the operating system.

Finally, `environment` has the streamer parameters, like webcam device from the container. In this case both the OS and the container match the path `/dev/video0`. Here you also need to provide you webcam parameters, in my case I indicated that the resolution is 1280x960 at 30 frames per second, which is what corresponds to my Logitech C270. For other models you can consult the [supported cameras list page](https://community.octoprint.org/t/usb-webcams-known-to-work-with-mjpg-streamer/21149){:target= &quot;_bla￼nk&quot;}.

Once the `docker-compose.yml` file has been created with the information shown above, all you have to do is start the service with Docker:

```
root@octopi:~/octoprint# docker-compose up -d
  
Creating octoprint_geniuspro_1 ... done
```

With this we can access the OctoPrint web interface from the Raspberry Pi on the port that we had indicated,  in my case the url is http://192.168.68.102:5000:

![OctoPrint Web 1](/assets/images/octoprint-multi-web.jpg)

## Adding a second printer

To add a second printer, you just have to replicate the configuration you had before, but specifying the new printer and the corresponding video camera. So for my Artillery Sidewinder X2 this would be:

```
version: '2.4'

services:
  geniuspro:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5000:5000
      - 8080:8080
    devices:
      - /dev/ttyACM0:/dev/ttyACM0
      - /dev/video0:/dev/video0
    volumes:
      - /root/octoprint/geniuspro:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30
 
  sidewinder_x2:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5000:5000
      - 8080:8080
    devices:
      - /dev/ttyACM1:/dev/ttyACM0
      - /dev/video2:/dev/video0
    volumes:
      - /root/octoprint/geniuspro:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30

```

Take into account the phycal printer is identified as `/dev/ttyACM1` in the Raspberry , but the container internally it will be see it as `/dev/ttyACM0`. The same goes for the video camera, where it is physically identified as `/dev/video2` but internally the container will see it as `/dev/video0`.

Once the configuration has been created, the container must be run as follows: 

```
root@octopi:~/octoprint# docker-compose up -d
  
octoprint_geniuspro_1 is up-to-date
Creating octoprint_sidewinderx2_1 ... done
```

This will now allow access to the second instance of OctoPrint on port 5001:

![OctoPrint Web 2](/assets/images/octoprint-multi-web2.jpg)

## How to differentiate printers?
In the example above I used the devices `/dev/ttyACM0` and `/dev/ttyACM1` to reference my first printer, the Artillery Genius Pro, and my second printer, the Artillery SideWinder X2, but the operating system not always takes that order, since it will depend on which one you had turned on first. In order to identify the printers regardless of the connection order we can create a symbolic link using `udev` with the manufacturer and product information.

For this, you have first list the USB devices:

```
root@octopi:~/octoprint# lsusb
Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 001 Device 004: ID 046d:0825 Logitech, Inc. Webcam C270
Bus 001 Device 003: ID 046d:0825 Logitech, Inc. Webcam C270
Bus 001 Device 015: ID 0483:5740 STMicroelectronics Virtual COM Port
Bus 001 Device 016: ID 0483:5740 STMicroelectronics Virtual COM Port
Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
```

There are two **STMicroelectronics Virtual COM Port** devices here, corresponding to the two printers, but which is which? We are going to ask for the detail of these devices:
 
```
 root@octopi:~/octoprint# lsusb -v -d 0483:5740

Bus 001 Device 015: ID 0483:5740 STMicroelectronics Virtual COM Port
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               2.00
  bDeviceClass            2 Communications
  bDeviceSubClass         2 Abstract (modem)
  bDeviceProtocol         0
  bMaxPacketSize0        64
  idVendor           0x0483 STMicroelectronics
  idProduct          0x5740 Virtual COM Port
  bcdDevice            0.00
  iManufacturer           1 STMicroelectronics
  iProduct                2 ARTILLERY_RUBY CDC in FS Mode
  iSerial                 3 386F39543538  
...
  
Bus 001 Device 016: ID 0483:5740 STMicroelectronics Virtual COM Port
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               2.00
  bDeviceClass            2 Communications
  bDeviceSubClass         2 Abstract (modem)
  bDeviceProtocol         0
  bMaxPacketSize0        64
  idVendor           0x0483 STMicroelectronics
  idProduct          0x5740 Virtual COM Port
  bcdDevice            0.00
  iManufacturer           1 STMicroelectronics
  iProduct                2 ARTILLERY_RUBY CDC in FS Mode
  iSerial                 3 3594398F3538  
...
```

What differentiates one printer from another is the serial number of the device. Since the Artillery Genius Pro was initially mapped to `/dev/ttyACM0`, we can check what its serial number is:
 
```
root@octopi:~/octoprint# udevadm info -a -n /dev/ttyACM0 | grep serial
    ATTRS{serial}==&quot;3594398F3538&quot;
    ATTRS{serial}==&quot;0000:01:00.0&quot;
```

Now to see the serial code of the Artillery Sidewinder X2 you have to consult the device `/dev/ttyACM1`:

```

root@octopi:~/octoprint# udevadm info -a -n /dev/ttyACM1 | grep serial
    ATTRS{serial}==&quot;386F39543538&quot;
    ATTRS{serial}==&quot;0000:01:00.0&quot; 
```

For `udev` to recognize these rules we must create the following file `/etc/udev/rules.d/40-printers.rules` , with this content:
 
```
 # Genius Pro
KERNEL==&quot;ttyACM[0-9]*&quot;, SUBSYSTEM==&quot;tty&quot;, ATTRS{idVendor}==&quot;0483&quot;, ATTRS{idProduct}==&quot;5740&quot;, ATTRS{serial}==&quot;3594398F3538&quot;, SYMLINK=&quot;ttyGeniusPro&quot;

# Sidewinder X2
KERNEL==&quot;ttyACM[0-9]*&quot;, SUBSYSTEM==&quot;tty&quot;, ATTRS{idVendor}==&quot;0483&quot;, ATTRS{idProduct}==&quot;5740&quot;, ATTRS{serial}==&quot;386F39543538&quot;, SYMLINK=&quot;ttySidewinderX2&quot;
```
 
 Then `udev` must be restarted to take these changes:
  
```
root@octopi:~/octoprint# udevadm control --reload-rules &amp;&amp; udevadm trigger
```

This way the printers will be mapped as follows:
  
```
root@octopi:~/octoprint# ls -l  /dev/tty{GeniusPro,Sidewinder}*
lrwxrwxrwx 1 root root 7 Aug 22 21:10 /dev/ttyGeniusPro -&gt; ttyACM0
lrwxrwxrwx 1 root root 7 Aug 22 21:10 /dev/ttySidewinderX2 -&gt; ttyACM1
```

So the Docker configuration can be modified to reflect these changes:
  
```
version: '2.4'

services:
  geniuspro:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5000:5000
      - 8080:8080
    devices:
      - /dev/ttyGeniusPro:/dev/ttyACM0
      - /dev/video0:/dev/video0
    volumes:
      - /root/octoprint/geniuspro:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30

  sidewinderx2:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5001:5000
      - 8081:8080
    devices:
      - /dev/ttySidewinderX2:/dev/ttyACM0
      - /dev/video2:/dev/video0
    volumes:
      - /root/octoprint/sidewinderx2:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30  

```

## Webcams
Unlike printers which have different serial numbers, I have two **Logitech C270** webcams that are exactly the same model. How can we  differente them? How can we make the OS to take the right camera?

Again we do the analysis with `lsusb`:

```
root@octopi:~/octoprint# lsusb
Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 001 Device 004: ID 046d:0825 Logitech, Inc. Webcam C270
Bus 001 Device 003: ID 046d:0825 Logitech, Inc. Webcam C270
Bus 001 Device 015: ID 0483:5740 STMicroelectronics Virtual COM Port
Bus 001 Device 016: ID 0483:5740 STMicroelectronics Virtual COM Port
Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
```
Both cameras are on the same bus but identified as different USB devices, which we can take advantage of. In this case we will not use the product id to differentiate them but the USB device id. We'll be referencing the `video4linux` subsystem instead of `tty`, so we'll be using `KERNELS` instead of `KERNEL` to differentiate cameras. For example, to find out which USB device the camera in `/dev/video0` is on, we can run the following:

```
root@octopi:~/octoprint# udevadm info -a -p  $(udevadm info -q path -n /dev/video0) | grep KERNELS
    KERNELS==&quot;1-1.4:1.0&quot;
    KERNELS==&quot;1-1.4&quot;
    KERNELS==&quot;1-1&quot;
    KERNELS==&quot;usb1&quot;
    KERNELS==&quot;0000:01:00.0&quot;
    KERNELS==&quot;0000:00:00.0&quot;
    KERNELS==&quot;pci0000:00&quot;
    KERNELS==&quot;fd500000.pcie&quot;
    KERNELS==&quot;scb&quot;
    KERNELS==&quot;platform&quot;
```

For the other camera we do the same:

```
root@octopi:~/octoprint# udevadm info -a -p  $(udevadm info -q path -n /dev/video2) | grep KERNELS
    KERNELS==&quot;1-1.3:1.0&quot;
    KERNELS==&quot;1-1.3&quot;
    KERNELS==&quot;1-1&quot;
    KERNELS==&quot;usb1&quot;
    KERNELS==&quot;0000:01:00.0&quot;
    KERNELS==&quot;0000:00:00.0&quot;
    KERNELS==&quot;pci0000:00&quot;
    KERNELS==&quot;fd500000.pcie&quot;
    KERNELS==&quot;scb&quot;
    KERNELS==&quot;platform&quot;
```

Now with this information we can modify the `udev` rules as follows:

```
# Genius Pro
KERNEL==&quot;ttyACM[0-9]*&quot;, SUBSYSTEM==&quot;tty&quot;, ATTRS{idVendor}==&quot;0483&quot;, ATTRS{idProduct}==&quot;5740&quot;, ATTRS{serial}==&quot;3594398F3538&quot;, SYMLINK=&quot;ttyGeniusPro&quot;
SUBSYSTEM==&quot;video4linux&quot;, KERNELS==&quot;1-1.3&quot;, ATTRS{idVendor}==&quot;046d&quot;, ATTRS{idProduct}==&quot;0825&quot;, SYMLINK=&quot;videoGeniusPro&quot;

# Sidewinder X2
KERNEL==&quot;ttyACM[0-9]*&quot;, SUBSYSTEM==&quot;tty&quot;, ATTRS{idVendor}==&quot;0483&quot;, ATTRS{idProduct}==&quot;5740&quot;, ATTRS{serial}==&quot;386F39543538&quot;, SYMLINK=&quot;ttySidewinderX2&quot;
SUBSYSTEM==&quot;video4linux&quot;, KERNELS==&quot;1-1.4&quot;, ATTRS{idVendor}==&quot;046d&quot;, ATTRS{idProduct}==&quot;0825&quot;, SYMLINK=&quot;videoSidewinderX2&quot;
```

Remember to reset the `udev` rules with:

```
root@octopi:~/octoprint# udevadm control --reload-rules &amp;&amp; udevadm trigger
```

So now we have the cameras mapped to `/dev/videoGeniusPro` and `/dev/videoSidewinderX2` respectively. Now we can modify the **docker-compose.yml** file to take this new configuration:

```
version: '2.4'

services:
  geniuspro:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5000:5000
      - 8080:8080
    devices:
      - /dev/ttyGeniusPro:/dev/ttyACM0
      - /dev/videoGeniusPro:/dev/video0
    volumes:
      - /root/octoprint/geniuspro:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30

  sidewinderx2:
    image: octoprint/octoprint
    restart: unless-stopped
    ports:
      - 5001:5000
      - 8081:8080
    devices:
      - /dev/ttySidewinderX2:/dev/ttyACM0
      - /dev/videoSidewinderX2:/dev/video0
    volumes:
      - /root/octoprint/sidewinderx2:/octoprint
    # uncomment the lines below to ensure camera streaming is enabled when
    # you add a video device
    environment:
      - ENABLE_MJPG_STREAMER=true
      - CAMERA_DEV=/dev/video0
      - MJPG_STREAMER_INPUT=&quot;-y -n -r 1280x960 -f 30
```

## Performance with two instances of OctoPrint
Probably you are wondering about the consumption of these containers in the operating system and if the Raspberry can handle it. Here is an image of `htop` with the two containers above printing to each of the printers at the same time.

![htop](/assets/images/octoprint-htop.jpg)

As you can see the Raspberry has a lot of resources available, since of the 8GB of RAM it is only consuming 0.5 GB including the operating system. From this we can draw two conclusions: the first one is that more printers can be added without problems, the only limit is the number of USB ports offered by the Raspberry (4 in my model), but you could add a USB hub to add more ports; and the second conclusion is that for two printers you could use a Raspberry with less RAM, for example 2 GB or 4 GB, and thus you would save a little.

## What does the Raspberry Pi and printers look like?
To finish this article I leave you a photo of my printers connected to the RaspBerry Pi:

![3D printers](/assets/images/3dprinters.jpg)

On the smaller printer you can see a small gray box that is the Raspberry Pi, next to the two web cameras.

# References
* [OctoPrint](https://octoprint.org){:target=&quot;_blank&quot;}
* [OctoPi](https://github.com/guysoft/OctoPi){:target=&quot;_blank&quot;}
* [Setting up OctoPrint on a Raspberry Pi for multiple printers](http://thomas-messmer.com/index.php/14-free-knowledge/howtos/79-setting-up-octoprint-for-multiple-printers){:target=&quot;_blank&quot;}
* [Installing OctoPrint using Docker in Linux (video tutorial)](https://youtu.be/LcA9o6OGfEg){:target=&quot;_blank&quot;}
* [Installing OctoPrint using Docker in Linux (instructions)](https://docs.google.com/document/d/1aU7LGYAe6r45LqEBQ8opuXCiNKeJl3XrPTrobvEvdOM/edit){:target=&quot;_blank&quot;}
* [USB webcams known to work with mjpg-streamer](https://community.octoprint.org/t/usb-webcams-known-to-work-with-mjpg-streamer/21149){:target=&quot;_bla￼nk&quot;}.
* [How to distinguish between identical USB-to-serial adapters?](https://askubuntu.com/questions/49910/how-to-distinguish-between-identical-usb-to-serial-adapters){:target=&quot;_blank&quot;}
* [Udev Rule to discern 2 identical webcams on Linux](https://unix.stackexchange.com/questions/424887/udev-rule-to-discern-2-identical-webcams-on-linux){:target=&quot;_blank&quot;}</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2022-04-17-paperlike-pencil-grips.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2022-04-17-paperlike-pencil-grips.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2022-04-17-paperlike-pencil-grips.md/">![PaperLike Pencil Grips](/assets/images/PaperLike-Pencil-Grip.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;
I've been using my iPad for a while to take notes with the Apple Pencil and I decided to write this article to tell you about a product that has pleasantly surprised me and which I think is an excellent improvement to consider: the **Paper Like Pencil Grips**.

It may have happened to you that when using the Apple Pencil for a long time, your hand gets tired? This is common even with traditional pencils, and for this there was already a solution, the classic grips adapted to the pencil to improve the posture of the fingers when writing. This is what the PaperLike team ended up designing, but for the Apple Pencil.

You'll say that they are simple grips and probably you think you can use traditional grips to adapt them to the Apple Pencil...Yes, for little money you can put a generic grip on it but there're a couple of things that you should consider, that the PaperLike team has already solved and that you won't have with a generic product:

## Apple Pencil's magnetic charge
![Blink config](/assets/images/Grip-on-iPad.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}

They achieve this thanks to the fact their grips are specially designed for the Apple Pencil, since they have the shape of the pencil, and the side used for magnetic charging is slightly thinner.

## The posture of the fingers
The PaperLike guys took into account two edge cases: precision and prolonged periods of use. Yes, two different pencil grips come in the envelop: The first is aimed at having maximum precision when grabbing the pencil, for example when you need fine details when drawing. The second one is designed to help those who take notes for a long time, ideal for university students.

In my case, I really enjoy the grip for long session using (although I left behind my university yeas a long time ago), but I am sure this will be very useful for students, and for artists (professional or not) it will be help them  to have more precise lines when making their creations.

It is definitely an improvement that undoubtedly changes the experience of using the iPad and the Apple Pencil.

## Double tap funtion
The Apple Pencil has a second function that is activated by double-tapping the flat part of the pencil. With a generic grip there's no way to double tap. The PaperLike Pencil Grips achieves this with the thinnest part of the grip. But I must comment that to use it you have to turn the pencil to be able to double tap. I use this a lot in **GoodNotes 5**, and it's not that natural. In fact, the founder of PaperLike in the How-To video comments that one option is to use the grip with that part facing up, but I think it loses its grace becuase the grip's shape was designed for the index fingers and the thumb. It seemed to me that the best option is to get used to turning the pencil before doing the double tap, and turning it  back to continue writing.

# The packaging
Finally, I would like to highlight something pleasant, which is the packaging in which it came. As I bought the grips at an early stage, they came in a paper envelope, which in fact they make clear is not the final packaging for the product.

![PaperLike Pencil Grips Packing](/assets/images/PaperLike-Pencil-Grips-Envelop.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}

In particular, it doesn’t bother me at all and in fact I would even say that it should be the final version, a simple and environmentally friendly presentation. The only thing that would suggest to improve is that the material of the envelope, it should be a little harder so that it can withstand the onslaught of the trip, or in any case they should send them in “fragile” correspondence, since the shipment was not at all economical (€ 39.99). In my case, mine arrived in that envelope a little wrinkled, and the contents also wrinkled.

Other details that I liked were the thank you note they place on the packaging, handwritten by the founder of PaperLike (and then printed industrially), as well as a very nice sticker that they add from the community of artists who use their products.

![PaperLike Pencil Grips thanking note](/assets/images/PaperLike-Pencil-Grips-thanks.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}

![PaperLike Pencil Grips sticker gift](/assets/images/PaperLike-Pencil-Grips-Sticker.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}

# Where to buy the PaperLike Grips?
As I mentioned, they are at an early stage, so at the moment the only place you will be able to buy them in PaperLike's store, got to [iPad accessories](https://paperlike.com/collections/ipad-accessories){:target=&quot;_blank&quot;} , here the [PaperLike Pencil Grips are available.](https://paperlike.com/collections/ipad-accessories/products/pencil-grip-set){:target=&quot;_blank&quot;}

# Conclusion
If you need a grip for your Apple Pencil, keep in mind this option offered by the PaperLike guys, which will undoubtedly improve your user experience on your iPad  significantly.

##  Disclaimer
I wrote this post to share my experience with the PaperLike pencil grips, and I did not receive any sponsorship or have any profit or participation in their sales.

# References
* [PaperLike page](https://paperlike.com){:target=&quot;_blank&quot;}
* [iPad Accessories](https://paperlike.com/collections/ipad-accessories){:target=&quot;_blank&quot;}
* [PaperLike Pencil Grips](https://paperlike.com/collections/ipad-accessories/products/pencil-grip-set){:target=&quot;_blank&quot;}</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2022-01-25-ipad-pro-as-a-portable-workstation.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2022-01-25-ipad-pro-as-a-portable-workstation.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2022-01-25-ipad-pro-as-a-portable-workstation.md/">![iPad Pro](/assets/images/ipad-pro-2021.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}&lt;br/&gt;
It's been a while since I posted on the blog, first because I couldn't find something interesting to share and second because there wasn't much spare time (It's not that now there's a lot of time to spare but well, one make some time to write a few lines).

The truth is I recently got an iPad Pro 11&quot; and among the things I tried was the possibility to use it as a light work equipment, that is, a computer that I can have on hand on my bedside table without it being a complete computer.

While the iPad has many advantages and things that I really liked, it seems the term &quot;Pro&quot; is not enough to have a complete terminal as you can find in an operating system or to have a functional [virtual machine](https://youtu.be/LrLDKYFyLMM){:target=&quot;_blank&quot;}, so I decided to investigate if there is an option to use it as a client that connects somewhere else where I can have access to a complete operating system.

In fact, I read an [article](https://arslan.io/2019/01/07/using-the-ipad-pro-as-my-development-machine/){:target=&quot;_blank&quot;} that suggested using a Digital Ocean Droplet to have a virtual machine in the cloud and from there be able to work. This option is not bad, but having a workstation at home I didn't see the need to go to the cloud.

Following some of these suggestions I ended up installing **Blink Shell** that comes with `mosh` support, and using `tmux` on my remote Linux laptop I was able to achieve this. Below I explain the details of this configuration.

# Blink Shell (ssh / mosh)

The first thing I had to look for was a decent `ssh` client for iPad. **[Blink Shell](https://blink.sh){:target=&quot;_blank&quot;}** appeared as recurring recommendation on Internet articles. At first, I didn't like the fact you have to buy it without a trial but since there were several recommendations I gave it a try (USD 19.99 at the date of publication of this post).

I have to admit  **Blink Shell** is very, very good, it's  designed for mobile devices and has support for `mosh`.

## Mosh
Maybe you are wondering as I did, What is `mosh`?  The definition from the `mosh` page says:

&gt; Remote terminal application that allows roaming, supports intermittent connectivity, and provides intelligent local echo and line editing of user keystrokes.

Do you still not understand what `mosh` is for as it happened to me? It's like `screen` but without having to learn commands to create and manage sessions, it also uses `ssh` under the hood. In other words, you will have the possibility to continue working on your session where you left it off, or if for example your connection was cut and you manage to connect again to another network.

As under the hood  it uses `ssh`, in order to use `mosh` the syntax is practically the same. Therefore if you used to connect to a server using `ssh` like this:

```
ssh lgallard@192.168.1.65
```

Now with `mosh` you can connect to the server as follows:

```
mosh lgallard@192.168.1.65
```

The change seems simple and with this we now are able to close our iPad or the computer where we are using the `mosh` client and be able to magically recover the work session when we reconnect.

You can configure  **Blink Shell** options by accessing a configuration wizard to have different hosts and ease the connection with a name instead of an IP address. To access this wizard simply type `config` from the terminal:

![Blink config](/assets/images/Blink_config.png){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}

In my case now I can connect to my workstation as follows:

```
mosh dauntless
```

## Mosh, the bad

As a con `mosh` can't scroll the terminal, that is, if you need to check the output of commands that you have previously executed and are no longer on the screen, you won't be able to see them. For me, this was a big problem because when using `terraform` I lost the plans with the changes to be applied.

This is the way `mosh` works, to have optimal performance they render the text. There's an [open issue](https://github.com/mobile-shell/mosh/issues/122){:target=&quot;_blank&quot;} since several years where the developers commented they left it opened as a reference but  they have no intention of implementing it. So, is there  no solution? Yes, `tmux`.

# What is tmux?

`tmux` is a tool that allows you to have several terminals open (or windows) to be accessed and controlled from a single terminal as we would do with `screen`.

You have to install `tmux` on the computer where we want to log in and once installed it's enough to run this from the `mosh` client (the iPad Pro in my case):

```
mosh danuntless -- tmux new  -s terminal-1
```

This way we can create a new session whose name is **terminal-1**. Then to access this session later you have to run the following:

`mosh danuntless -- tmux a  -t terminal-1`

## Enabling  scrolling on tmux

On the remote workstation you must configure `mosh` to have scrolling enabled using `tmux`, to achieve this edit the  `~/.tmux.conf` file with this content:

```
new-session
set -g history-limit 30000
set -g mouse on
```

## Blink + ssh  + mosh + tmux ... eh?

&quot;Wait a moment ... **Blink + ssh + mosh + tmux**, Are all these too complicated? Isn't it better to use `ssh` with `screen`? &quot;. Yes, it's an alternative, but with `ssh` you would not be able to reconnect automatically and you will have to log in again if you close the iPad or if your IP address changes for some reason (for example,  if you change  your connection from WiFI to cell phone) or if the `ssh` session expires.

I have to admit there's a learning curve to cover with `tmux` but once passed you will love it and wonder why you didn't use `mosh` and `tmux` before.

## More about tmux

If you want to learn how to use `tmux` I leave you a [cheat sheet](https://tmuxcheatsheet.com){:target=&quot;_blank&quot;} with the commands you can use. In my case, I found the panels division and the use of zooming between them very useful, but I must admit that copying text between panels is somewhat complicated on the iPad and sometimes I prefer to use `vim` for this, but I will surely find a workaround.

# Final words

Here I comment my experiences with the iPad Pro, but all these can be replicated on an iPad Air and even use `mosh` and `tmux` from other workstations with Linux or Windows.

# References

* [Run ANY OS on iPad or iPhone! - YouTube](https://youtu.be/LrLDKYFyLMM){:target=&quot;_blank&quot;}
* [Using the iPad Pro as my development machine](https://arslan.io/2019/01/07/using-the-ipad-pro-as-my-development-machine/){:target=&quot;_blank&quot;}{:target=&quot;_blank&quot;}
* [Blink Shell](https://blink.sh){:target=&quot;_blank&quot;}
* [Mosh](https://blink.sh){:target=&quot;_blank&quot;}
* [Tmux cheat sheet](https://tmuxcheatsheet.com)
* [mosh prevents the use of scrollback](https://github.com/mobile-shell/mosh/issues/122){:target=&quot;_blank&quot;}</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2021-06-15-helm3-local-repo.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2021-06-15-helm3-local-repo.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2021-06-15-helm3-local-repo.md/">![Helm 3 local repo](/assets/images/helm.jpg){:style=&quot;display:block; margin-left:auto; margin-right:auto&quot;}
In Helm 3 the support of **helm serve** command was removed due to some design issues, therefore if you need a similar tool you have to install  **helm servecm**  plugin, which uses **ChartMuseum** for publishing the charts to your local storage (other storages are also supported, like S3 buckets).

In this post you will learn how to install and publish a local repo in Helm 3.

1. Install **ChartMuseum**&lt;br/&gt;
   First off, you need to install **ChartMuseum**. At the project page you will find different ways  of installing it, but I install as a Go app as follows:&lt;br/&gt;&lt;br/&gt;
   ```bash
   GO111MODULE=&quot;on&quot; go get github.com/helm/chartmuseum@v0.13.1
   ```
   
2. Install **helm servecm plugin**&lt;br/&gt;

   &lt;br/&gt;Then you need to install **servecm** as a helm plugin:&lt;br/&gt;&lt;br/&gt;

   ```bash
   helm plugin install https://github.com/jdolitsky/helm-servecm
   ```

3. Install **helm push plugin**&lt;br/&gt;
   In order to host your charts, you will be using **ChartMuseum** and **helm servecm** pluing, but you will still need to publish them into **ChartMuseum**. You can do it manually or use another plugin called **helm push** which does it for you:&lt;br/&gt;&lt;br/&gt;

   ```bash
   helm plugin install https://github.com/chartmuseum/helm-push.git
   ```

4. Add the local repo in helm:&lt;br/&gt;&lt;br/&gt;

   ```
   helm repo add local http://127.0.0.1:8879/charts
   ```
5. Run **helm servecm plugin**:

   &lt;br/&gt;Next step is to run **helm servecm** plugin:&lt;br/&gt;&lt;br/&gt;

   ```bash
   helm servecm --port=8879 --storage local --storage-local-rootdir ./local --context-path=/charts 
   ```

   Now you can publish your charts at http://127.0.0.1:8879/charts

6. Publish your chart to your local repo:

   ```bash
   helm push your-chart local
   ```

At this point you will be able to use you local charts in your cluster by executing:

```bash
helm install  your-chart
```

## References
* [Removal of helm serve](https://helm.sh/docs/faq/#removal-of-helm-serve){:target=&quot;_blank&quot;}
* [helm servecm plugin](https://github.com/jdolitsky/helm-servecm){:target=&quot;_blank&quot;}
* [ChartMuseum](https://github.com/helm/chartmuseum){:target=&quot;_blank&quot;}
* [helm push plugin](https://github.com/chartmuseum/helm-push){:target=&quot;_blank&quot;}</content><author><name>Luis M. Gallardo D.</name></author></entry><entry><title type="html"></title><link href="https://lgallardo.com/_i18n/en/2021-04-15-terraform-module-for-aws-ecr.md/" rel="alternate" type="text/html" title="" /><published>2025-06-09T07:17:29-05:00</published><updated>2025-06-09T07:17:29-05:00</updated><id>https://lgallardo.com/_i18n/en/2021-04-15-terraform-module-for-aws-ecr.md</id><content type="html" xml:base="https://lgallardo.com/_i18n/en/2021-04-15-terraform-module-for-aws-ecr.md/">&lt;center&gt;&lt;img src=&quot;/images/terraform-aws-ecr.jpg&quot; alt=&quot;Terraform&quot; /&gt;&lt;/center&gt;&lt;br/&gt;
I share here another Terraform module I published as open source code, which allows you to create registries in AWS ECR.

You can check my module **terraform-aws-ecr** at the [Terraform Registry](https://registry.terraform.io/modules/lgallard/ecr/aws){:target=&quot;_blank&quot;} or clone it from [Github](https://github.com/lgallard/terraform-aws-ecr.git){:target=&quot;_blank&quot;}.

If you want to take a sneak of the module, I also left the README in this post:

![Terraform](https://lgallardo.com/images/terraform.jpg){:target=&quot;_blank&quot;}

# terraform-aws-ecr
Terraform module to create [AWS ECR](https://aws.amazon.com/ecr/){:target=&quot;_blank&quot;} (Elastic Container Registry) which is a fully-managed Docker container registry.

## Usage
You can use this module to create an ECR registry using few parameters (simple example) or define in detail every aspect of the registry (complete example).

Check the [examples](https://github.com/lgallard/terraform-aws-ecr/tree/master/examples){:target=&quot;_blank&quot;} for the  **simple** and the **complete** snippets.

### Simple example
This example creates an ECR registry using few parameters

```
module &quot;ecr&quot; {

  source = &quot;lgallard/ecr/aws&quot;

  name         = &quot;ecr-repo-dev&quot;

  # Tags
  tags = {
    Owner       = &quot;DevOps team&quot;
    Environment = &quot;dev&quot;
    Terraform   = true
  }

}
```

### Complete example
In this example the register is defined in detailed.

```
module &quot;ecr&quot; {

  source = &quot;lgallard/ecr/aws&quot;

  name                 = &quot;ecr-repo-dev&quot;
  scan_on_push         = true
  timeouts_delete      = &quot;60m&quot;
  image_tag_mutability = &quot;MUTABLE&quot;


  # Note that currently only one policy may be applied to a repository.
  policy = &lt;&lt;EOF
{
    &quot;Version&quot;: &quot;2008-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;repo policy&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: &quot;*&quot;,
            &quot;Action&quot;: [
                &quot;ecr:GetDownloadUrlForLayer&quot;,
                &quot;ecr:BatchGetImage&quot;,
                &quot;ecr:BatchCheckLayerAvailability&quot;,
                &quot;ecr:PutImage&quot;,
                &quot;ecr:InitiateLayerUpload&quot;,
                &quot;ecr:UploadLayerPart&quot;,
                &quot;ecr:CompleteLayerUpload&quot;,
                &quot;ecr:DescribeRepositories&quot;,
                &quot;ecr:GetRepositoryPolicy&quot;,
                &quot;ecr:ListImages&quot;,
                &quot;ecr:DeleteRepository&quot;,
                &quot;ecr:BatchDeleteImage&quot;,
                &quot;ecr:SetRepositoryPolicy&quot;,
                &quot;ecr:DeleteRepositoryPolicy&quot;
            ]
        }
    ]
}
EOF

  # Only one lifecycle policy can be used per repository.
  # To apply multiple rules, combined them in one policy JSON.
  lifecycle_policy = &lt;&lt;EOF
{
    &quot;rules&quot;: [
        {
            &quot;rulePriority&quot;: 1,
            &quot;description&quot;: &quot;Expire untagged images older than 14 days&quot;,
            &quot;selection&quot;: {
                &quot;tagStatus&quot;: &quot;untagged&quot;,
                &quot;countType&quot;: &quot;sinceImagePushed&quot;,
                &quot;countUnit&quot;: &quot;days&quot;,
                &quot;countNumber&quot;: 14
            },
            &quot;action&quot;: {
                &quot;type&quot;: &quot;expire&quot;
            }
        },
        {
            &quot;rulePriority&quot;: 2,
            &quot;description&quot;: &quot;Keep last 30 dev images&quot;,
            &quot;selection&quot;: {
                &quot;tagStatus&quot;: &quot;tagged&quot;,
                &quot;tagPrefixList&quot;: [&quot;dev&quot;],
                &quot;countType&quot;: &quot;imageCountMoreThan&quot;,
                &quot;countNumber&quot;: 30
            },
            &quot;action&quot;: {
                &quot;type&quot;: &quot;expire&quot;
            }
        }
    ]
}
EOF

  # Tags
  tags = {
    Owner       = &quot;DevOps team&quot;
    Environment = &quot;dev&quot;
    Terraform   = true
  }

}

```
## Providers

| Name | Version |
|------|---------|
| aws | n/a |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| encryption\_type | The encryption type to use for the repository. Valid values are `AES256` or `KMS` | `string` | `&quot;AES256&quot;` | no |
| image\_scanning\_configuration | Configuration block that defines image scanning configuration for the repository. By default, image scanning must be manually triggered. See the ECR User Guide for more information about image scanning. | `map` | `{}` | no |
| image\_tag\_mutability | The tag mutability setting for the repository. Must be one of: `MUTABLE` or `IMMUTABLE`. | `string` | `&quot;MUTABLE&quot;` | no |
| kms\_key | The ARN of the KMS key to use when encryption\_type is `KMS`. If not specified when encryption\_type is `KMS`, uses a new KMS key. Otherwise, uses the default AWS managed key for ECR. | `string` | n/a | no |
| lifecycle\_policy | Manages the ECR repository lifecycle policy | `string` | n/a | yes |
| name | Name of the repository. | `string` | n/a | yes |
| policy | Manages the ECR repository policy | `string` | n/a | yes |
| scan\_on\_push | Indicates whether images are scanned after being pushed to the repository (true) or not scanned (false). | `bool` | `true` | no |
| tags | A mapping of tags to assign to the resource. | `map(string)` | `{}` | no |
| timeouts | Timeouts map. | `map` | `{}` | no |
| timeouts\_delete | How long to wait for a repository to be deleted. | `string` | n/a | no |

## Outputs

| Name | Description |
|------|-------------|
| arn | Full ARN of the repository |
| name | The name of the repository. |
| registry\_id | The registry ID where the repository was created. |
| repository\_url | The URL of the repository (in the form `aws_account_id.dkr.ecr.region.amazonaws.com/repositoryName`) |

## References

* [terraform-aws-ecr module at Github](https://github.com/lgallard/terraform-aws-ecr.git){:target=&quot;_blank&quot;}
* [terraform-aws-ecr module at Terraform Registry](https://registry.terraform.io/modules/lgallard/ecr/aws){:target=&quot;_blank&quot;}</content><author><name>Luis M. Gallardo D.</name></author></entry></feed>